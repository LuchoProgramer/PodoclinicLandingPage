# ü§ñ Robots.txt - Gu√≠a R√°pida de Referencia

**Archivo**: `/public/robots.txt` | **Validador**: `/scripts/validate-robots.js`

---

## ‚ö° **Comandos Esenciales**

```bash
# Validar robots.txt completo
node scripts/validate-robots.js

# Probar URL espec√≠fica
node scripts/validate-robots.js "/ruta/a/probar"

# Ver robots.txt actual  
cat public/robots.txt

# Verificar en producci√≥n
curl https://podoclinicec.com/robots.txt
```

---

## üìù **Estructura del Archivo Actual**

```plaintext
# Comentarios descriptivos
User-agent: *           # Todos los crawlers
Allow: /               # Permite todo por defecto

# Bloqueos espec√≠ficos
Disallow: /api/        # APIs (no indexar)
Disallow: /_next/      # Archivos Next.js  
Disallow: /scripts/    # Scripts t√©cnicos

# Permisos expl√≠citos SEO
Allow: /blog/          # Blog (prioritario)
Allow: /servicios/     # Servicios (prioritario)
Allow: /faq           # FAQ (prioritario)
Allow: /tips/         # Tips (prioritario)

# Crawlers espec√≠ficos
User-agent: Googlebot  # Google
Allow: /

User-agent: Bingbot    # Bing/Microsoft
Allow: /

# Sitemap
Sitemap: https://podoclinicec.com/sitemap.xml
```

---

## üéØ **Testing R√°pido**

### **URLs que DEBEN estar permitidas**:
```bash
node scripts/validate-robots.js "/"                          # ‚úÖ Home
node scripts/validate-robots.js "/blog/uneros"              # ‚úÖ Blog  
node scripts/validate-robots.js "/servicios/pie-diabetico"  # ‚úÖ Servicios
node scripts/validate-robots.js "/faq"                      # ‚úÖ FAQ
node scripts/validate-robots.js "/tips/uneros"              # ‚úÖ Tips
```

### **URLs que DEBEN estar bloqueadas**:
```bash
node scripts/validate-robots.js "/api/chat"                 # ‚ùå API
node scripts/validate-robots.js "/_next/static/css/app.css" # ‚ùå Next.js
node scripts/validate-robots.js "/scripts/validate-robots.js" # ‚ùå Scripts
```

---

## üîß **Modificaciones Comunes**

### **Agregar nuevo bloqueo**:
```plaintext
Disallow: /nueva-ruta/
```

### **Permitir expl√≠citamente nueva secci√≥n**:
```plaintext
Allow: /nueva-seccion/
```

### **Agregar crawler espec√≠fico**:
```plaintext
User-agent: NombreCrawler
Allow: /
```

### **Cambiar sitemap**:
```plaintext
Sitemap: https://podoclinicec.com/nuevo-sitemap.xml
```

---

## ‚ö†Ô∏è **Errores Comunes a Evitar**

| ‚ùå Error | ‚úÖ Correcto |
|----------|-------------|
| `Disallow /admin` | `Disallow: /admin` |
| `User-agent *` | `User-agent: *` |
| `http://sitemap.xml` | `https://domain.com/sitemap.xml` |
| `# *` (comentario mal) | `# Comentario descriptivo` |
| `Host: domain.com` | No usar (deprecado) |

---

## üìä **Validaci√≥n Exitosa**

**Output esperado del validador**:
```
‚úÖ ¬°Robots.txt es v√°lido y est√° bien optimizado!
üìà RESUMEN:
   ‚Ä¢ Errores: 0
   ‚Ä¢ Advertencias: 0
   ‚Ä¢ Sugerencias: 0
   ‚Ä¢ Estado: ‚úÖ V√°lido para producci√≥n
```

---

## üö® **Checklist Antes de Cambios**

- [ ] Backup del robots.txt actual
- [ ] Probar en desarrollo primero  
- [ ] Ejecutar validador
- [ ] Probar URLs cr√≠ticas
- [ ] Verificar sintaxis

## üì§ **Checklist Despu√©s de Cambios**

- [ ] Validar con script
- [ ] Reenviar a Google Search Console
- [ ] Actualizar en Bing Webmaster Tools
- [ ] Monitorear indexaci√≥n (1-2 semanas)
- [ ] Verificar en producci√≥n

---

**üéØ Resultado**: Robots.txt optimizado para m√°ximo SEO con 0 errores