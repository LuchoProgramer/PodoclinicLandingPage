#!/usr/bin/env node

/**
 * ü§ñ TEMPLATE UNIVERSAL - Validador robots.txt  
 * 
 * INSTRUCCIONES DE USO:
 * 1. Copia este archivo a tu nuevo proyecto: examples/robots-validator.js
 * 2. Copia tambi√©n: scripts/validate-robots.js 
 * 3. Personaliza las URLs en la secci√≥n "PERSONALIZAR AQU√ç"
 * 4. Ejecuta: node examples/robots-validator.js
 * 
 * Creado por: Sistema SEO Podoclinicec
 * Versi√≥n: 1.0 - Noviembre 2025
 */

import { RobotsValidator } from '../scripts/validate-robots.js';
import fs from 'fs';

// ================================
// PERSONALIZAR AQU√ç - CONFIGURACI√ìN DEL PROYECTO
// ================================

const PROJECT_CONFIG = {
  name: 'Mi Proyecto',  // üëà CAMBIAR: Nombre de tu proyecto
  domain: 'mi-dominio.com',  // üëà CAMBIAR: Tu dominio
  industry: 'General',  // üëà CAMBIAR: Tu industria (e-commerce, blog, saas, etc.)
};

// üëà PERSONALIZAR: URLs espec√≠ficas de tu proyecto
const TEST_CASES = [
  // ========== RUTAS P√öBLICAS (deben estar permitidas) ==========
  { url: '/', description: 'P√°gina principal', expected: true },
  { url: '/about', description: 'Acerca de nosotros', expected: true },
  { url: '/contact', description: 'Contacto', expected: true },
  { url: '/services', description: 'Servicios', expected: true },
  { url: '/blog', description: 'Blog', expected: true },
  { url: '/products', description: 'Productos', expected: true },
  
  // üëà AGREGAR M√ÅS RUTAS P√öBLICAS AQU√ç
  
  // ========== RUTAS PRIVADAS/T√âCNICAS (deben estar bloqueadas) ==========
  { url: '/api/users', description: 'API usuarios', expected: false },
  { url: '/api/orders', description: 'API pedidos', expected: false },
  { url: '/admin', description: 'Panel administrador', expected: false },
  { url: '/dashboard', description: 'Dashboard usuarios', expected: false },
  { url: '/_next/static/css/app.css', description: 'Archivos t√©cnicos', expected: false },
  
  // üëà AGREGAR M√ÅS RUTAS PRIVADAS AQU√ç
];

// üëà PERSONALIZAR: Crawlers relevantes para tu industria
const CRAWLERS_TO_TEST = ['*', 'Googlebot', 'Bingbot', 'DuckDuckBot'];

// üëà PERSONALIZAR: Mejores pr√°cticas espec√≠ficas de tu industria
const BEST_PRACTICES = [
  `‚úÖ Comentarios descriptivos para ${PROJECT_CONFIG.industry}`,
  '‚úÖ User-agent: * como regla principal', 
  '‚úÖ Allow: / para permitir por defecto',
  '‚úÖ Bloqueos espec√≠ficos solo para APIs y contenido sensible',
  '‚úÖ Permisos expl√≠citos para contenido SEO importante',
  '‚úÖ Reglas espec√≠ficas para crawlers principales',
  '‚úÖ Sitemap URL completa y v√°lida',
  '‚úÖ Sin directivas deprecadas o incorrectas',
  '‚úÖ Sintaxis perfecta en todas las l√≠neas',
  `‚úÖ Optimizado para ${PROJECT_CONFIG.industry} y SEO local`
];

// ================================
// NO MODIFICAR DEBAJO DE ESTA L√çNEA
// ================================

console.log(`ü§ñ Validador robots.txt - ${PROJECT_CONFIG.name}\n`);
console.log('=' .repeat(60));

// Ejemplo 1: Validar archivo actual
console.log('\nüìã VALIDACI√ìN DEL ARCHIVO ROBOTS.TXT');
console.log('-'.repeat(50));

const validator = new RobotsValidator();
const robotsPath = 'public/robots.txt';

if (!fs.existsSync(robotsPath)) {
  console.error('‚ùå No se encontr√≥ public/robots.txt');
  console.log('üí° Crea el archivo primero con contenido b√°sico:');
  console.log(`
User-agent: *
Allow: /

Disallow: /api/
Disallow: /_next/

Sitemap: https://${PROJECT_CONFIG.domain}/sitemap.xml
  `);
  process.exit(1);
}

validator.validateFile(robotsPath);

// Ejemplo 2: Testing de URLs cr√≠ticas
console.log('\nüîç TESTING DE URLs ESPEC√çFICAS DEL PROYECTO');
console.log('-'.repeat(50));

const robotsContent = fs.readFileSync(robotsPath, 'utf8');
let allTestsPassed = true;

TEST_CASES.forEach((testCase, index) => {
  const result = validator.testURL(robotsContent, testCase.url);
  const status = result === testCase.expected ? '‚úÖ CORRECTO' : '‚ùå ERROR';
  const access = result ? 'Permitida' : 'Bloqueada';
  
  if (result !== testCase.expected) {
    allTestsPassed = false;
  }
  
  console.log(`${index + 1}. ${testCase.description}`);
  console.log(`   URL: ${testCase.url}`);
  console.log(`   Resultado: ${access} ${status}`);
  console.log('');
});

// Ejemplo 3: Testing con diferentes crawlers
console.log('\nü§ñ TESTING CON DIFERENTES CRAWLERS');
console.log('-'.repeat(50));

const sampleUrl = TEST_CASES.find(t => t.expected === true)?.url || '/';
CRAWLERS_TO_TEST.forEach(crawler => {
  console.log(`Testing ${sampleUrl} con ${crawler}:`);
  validator.testURL(robotsContent, sampleUrl, crawler);
  console.log('');
});

// Ejemplo 4: Verificar mejores pr√°cticas
console.log('\nüéØ MEJORES PR√ÅCTICAS IMPLEMENTADAS');
console.log('-'.repeat(50));

const practiceChecks = [
  { 
    practice: 'Comentarios descriptivos', 
    check: () => robotsContent.includes(PROJECT_CONFIG.name) || robotsContent.includes('#')
  },
  { 
    practice: 'User-agent: * presente', 
    check: () => robotsContent.includes('User-agent: *')
  },
  { 
    practice: 'Allow: / por defecto', 
    check: () => robotsContent.includes('Allow: /')
  },
  { 
    practice: 'Bloqueos para APIs', 
    check: () => robotsContent.includes('Disallow: /api')
  },
  { 
    practice: 'Sitemap incluido', 
    check: () => robotsContent.includes('Sitemap:')
  },
  { 
    practice: 'Sin directiva Host deprecada', 
    check: () => !robotsContent.includes('Host:')
  }
];

practiceChecks.forEach((item, index) => {
  const status = item.check() ? '‚úÖ' : '‚ùå';
  console.log(`   ${status} ${item.practice}`);
});

// Resumen final
console.log('\nüìä RESUMEN FINAL');
console.log('-'.repeat(50));

if (allTestsPassed) {
  console.log('üéâ ¬°TODOS LOS TESTS PASARON!');
  console.log(`‚úÖ Robots.txt de ${PROJECT_CONFIG.name} est√° correctamente configurado`);
} else {
  console.log('‚ö†Ô∏è  ALGUNOS TESTS FALLARON');
  console.log('üîß Revisa las URLs marcadas con ‚ùå ERROR y ajusta tu robots.txt');
}

console.log(`\nüìà Estado: ${allTestsPassed ? 'LISTO PARA PRODUCCI√ìN' : 'REQUIERE AJUSTES'}`);

console.log('\nüí° COMANDOS √öTILES PARA ESTE PROYECTO:');
console.log('-'.repeat(50));
console.log('# Validar robots.txt');
console.log('node scripts/validate-robots.js');
console.log('');
console.log('# Ejecutar este validador completo'); 
console.log('node examples/robots-validator.js');
console.log('');
console.log('# Probar URL espec√≠fica');
console.log('node scripts/validate-robots.js "/tu-url-aqui"');
console.log('');
console.log('# Verificar en producci√≥n');
console.log(`curl https://${PROJECT_CONFIG.domain}/robots.txt`);

console.log('\nüéØ PR√ìXIMOS PASOS RECOMENDADOS:');
console.log('-'.repeat(50));
if (allTestsPassed) {
  console.log('1. Subir robots.txt a producci√≥n');
  console.log('2. Verificar acceso web: curl https://tu-dominio.com/robots.txt');
  console.log('3. Enviar sitemap a Google Search Console');
  console.log('4. Monitorear indexaci√≥n en 1-2 semanas');
} else {
  console.log('1. Corregir URLs marcadas con ‚ùå ERROR');
  console.log('2. Ejecutar validador nuevamente');
  console.log('3. Cuando todos los tests pasen, subir a producci√≥n');
}